defaults:
  - _self_
  - drone
  - ppo
  - sim

headless: False
device: "cuda:0"
seed: 0

# Total Training Length
max_frame_num: 12e8 # max frame 
eval_interval: 1000 # evaluate the policy for every N training steps
save_interval: 1000

# Training Environment
env:
  num_envs: 2 # Number of drones for training
  max_episode_length: 2200
  env_spacing: 8.0 
  num_obstacles: 200

  # ==================== whole-body ====================
  # Flight height constraints
  min_flight_height: 0.5         # Minimum allowed flight height (m)
  max_flight_height: 3.5         # Maximum allowed flight height (m)
  height_penalty_deadzone: 0.2   # Buffer zone before penalty applies (m)
  height_penalty_weight: 8.0     # Weight of height penalty in reward function
  height_penalty_huber_delta: 0.5 # Huber loss delta: transition point from L2 to L1 (m)
  
  # Safety reward parameters (exponential form: λ(1 - e^(-k*d²)))
  safety_reward_lambda: 1.0      # Maximum reward value at infinite distance
  safety_reward_k: 0.5           # Decay rate: larger k = faster saturation

  # Reach-goal bonus (one-time per episode)
  reach_goal_distance_threshold: 0.5  # meters, within this distance counts as goal reached
  reach_goal_bonus: 20.0              # scalar bonus added once when first reaching goal in an episode
  terminate_on_reach_goal: false      # whether to terminate the episode immediately upon reaching goal
  
  # Static horizontal beams configuration
  num_static_beams: 100             # Number of horizontal beams
  beam_length_range: [1.0, 10.0]    # Beam length range (m)
  beam_thickness_range: [0.5, 1.0]  # Beam thickness range (m)
  beam_height_range: [0.5, 4.0]     # Beam height above ground (m)
  # ==================== whole-body ====================

  # ==================== whole-body curriculum (reward weights) ====================
  curriculum_total_steps: 1_000_000   # Total effective steps to reach p=1.0
  curriculum_step_divisor: 1.0        # Divide raw env steps to slow progression if needed
  curriculum_r1_steps: 10000       
  curriculum_r2_steps: 40000    

  # Linear schedules for weights: w = start + (end - start) * p
  vel_weight_start: 2.5
  vel_weight_end: 1.5
  safety_weight_start: 0.5
  safety_weight_end: 1.0
  safety_dyn_weight_start: 0.5   # if dynamic obstacles enabled; defaults to safety_weight_* if absent
  safety_dyn_weight_end: 1.0
  curriculum_log_interval: 1000   # Log p and weights every N steps during training (<=0 disables)
  curriculum_freeze_in_eval: true  # Do not advance curriculum_step during evaluation episodes
  # ======================================================

env_dyn:
  num_obstacles: 80 # set to zero if static env is needed
  vel_range: [0.5, 1.5]
  local_range: [5.0, 5.0, 4.5]

viewer:
  eye: [0., 40., 40.]
  lookat: [0., 2.5, 0.]
  resolution: [960, 720]

wandb:
  project: wb
  name: wb
  entity: chai8226-sichuan-university
  mode: offline # online
  run_id: #kw9xf9k9
