defaults:
  - _self_
  - drone
  - ppo
  - sim

headless: False
device: "cuda:0"
seed: 0

# Total Training Length
max_frame_num: 12e8 # max frame 
eval_interval: 1000 # evaluate the policy for every N training steps
save_interval: 1000

# Training Environment
env:
  num_envs: 2 # Number of drones for training
  max_episode_length: 2200
  env_spacing: 8.0 
  num_obstacles: 200

  # ==================== whole-body ====================
  # Unified height reward: combines preference for an optimal height with penalties for being out of bounds
  height_reward_sigma: 0.5       # Std dev for Gaussian reward, controls how focused the reward is around optimal_height
  height_reward_weight: 1.0      # Weight for the combined height reward term
  height_penalty_weight: 8.0     # Weight for the height penalty term
  min_flight_height: 0.5         # Minimum allowed flight height (m)
  max_flight_height: 3.5         # Maximum allowed flight height (m)
  
  # Safety reward parameters (exponential form: λ(1 - e^(-k*d²)))
  safety_reward_lambda: 1.0      # Maximum reward value at infinite distance
  safety_reward_k: 0.5           # Decay rate: larger k = faster saturation

  reach_goal_weight: 10.0
  
  # Static horizontal beams configuration
  num_static_beams: 40              # Number of horizontal beams
  beam_length_range: [1.0, 8.0]    # Beam length range (m)
  beam_thickness_range: [0.3, 0.8]  # Beam thickness range (m)
  beam_height_range: [0.3, 5.0]     # Beam height above ground (m)
  # ==================== whole-body ====================

  # ==================== whole-body curriculum (reward weights) ====================
  curriculum_total_steps: 1_000_000   # Total effective steps to reach p=1.0
  curriculum_step_divisor: 1.0        # Divide raw env steps to slow progression if needed
  curriculum_r1_steps: 20000       
  curriculum_r2_steps: 50000    

  # Linear schedules for weights: w = start + (end - start) * p
  vel_weight_start: 2.5
  vel_weight_end: 1.5
  safety_weight_start: 0.5
  safety_weight_end: 1.0
  safety_dyn_weight_start: 0.5   # if dynamic obstacles enabled; defaults to safety_weight_* if absent
  safety_dyn_weight_end: 1.0
  curriculum_log_interval: 1000   # Log p and weights every N steps during training (<=0 disables)
  curriculum_freeze_in_eval: true  # Do not advance curriculum_step during evaluation episodes
  # ======================================================

env_dyn:
  num_obstacles: 80 # set to zero if static env is needed
  vel_range: [0.5, 1.5]
  local_range: [5.0, 5.0, 4.5]

viewer:
  eye: [0., 40., 40.]
  lookat: [0., 2.5, 0.]
  resolution: [960, 720]

wandb:
  project: wb
  name: wb
  entity: chai8226-sichuan-university
  mode: offline # online
  run_id: #kw9xf9k9
